{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f34046b",
   "metadata": {},
   "source": [
    "# Assignment Title: Web Scraping with Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b3d12",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "The objective of this assignment is to help trainees gain hands-on experience with Scrapy, a powerful web scraping framework in Python. By the end of this assignment, trainees should be able to create Scrapy projects, build spiders to extract data from websites, and store the scraped data in various formats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35191118",
   "metadata": {},
   "source": [
    "## Task 1: Install and Set Up Scrapy\n",
    "### Install Scrapy:\n",
    "  - Install Scrapy in your Python environment.\n",
    "  - Use the following command to install: pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4adc32f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Using cached Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n",
      "Collecting tldextract\n",
      "  Using cached tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: lxml>=4.4.1 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from scrapy) (4.6.3)\n",
      "Collecting parsel>=1.5.0\n",
      "  Using cached parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Using cached cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "  Using cached queuelib-1.7.0-py2.py3-none-any.whl (13 kB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.9.1 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow 2.9.1 requires tensorflow-io-gcs-filesystem>=0.23.1, which is not installed.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n",
      "tensorflow 2.9.1 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\n",
      "tensorflow 2.9.1 requires flatbuffers<2,>=1.12, but you have flatbuffers 2.0 which is incompatible.\n",
      "tensorflow 2.9.1 requires gast<=0.4.0,>=0.2.1, but you have gast 0.5.3 which is incompatible.\n",
      "tensorflow 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Using cached PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Using cached Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
      "Collecting cryptography>=36.0.0\n",
      "  Downloading cryptography-43.0.1-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Using cached w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting Twisted>=18.9.0\n",
      "  Using cached twisted-24.7.0-py3-none-any.whl (3.2 MB)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from scrapy) (21.0.0)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Using cached itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\rista\\anaconda3\\lib\\site-packages (from scrapy) (21.3)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rista\\anaconda3\\lib\\site-packages (from scrapy) (58.0.4)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Using cached itemloaders-1.3.1-py3-none-any.whl (12 kB)\n",
      "Collecting service-identity>=18.1.0\n",
      "  Using cached service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->scrapy) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rista\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.20)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from pyOpenSSL>=21.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (21.2.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\rista\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Collecting incremental>=24.7.0\n",
      "  Using cached incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Using cached constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Collecting attrs>=19.1.0\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Collecting automat>=0.8.0\n",
      "  Using cached Automat-24.8.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (3.2)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-74.1.2-py3-none-any.whl (1.3 MB)\n",
      "Collecting tomli\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from packaging->scrapy) (3.0.4)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.3.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.26.0)\n",
      "Collecting requests-file>=1.4\n",
      "  Using cached requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rista\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.7)\n",
      "Installing collected packages: w3lib, typing-extensions, tomli, setuptools, jmespath, cssselect, requests-file, parsel, itemadapter, incremental, hyperlink, cryptography, constantly, automat, attrs, Twisted, tldextract, service-identity, queuelib, PyDispatcher, protego, itemloaders, scrapy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 58.0.4\n",
      "    Uninstalling setuptools-58.0.4:\n",
      "      Successfully uninstalled setuptools-58.0.4\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 3.4.8\n",
      "    Uninstalling cryptography-3.4.8:\n",
      "      Successfully uninstalled cryptography-3.4.8\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-24.7.0 attrs-24.2.0 automat-24.8.1 constantly-23.10.4 cryptography-43.0.1 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.9.0 itemloaders-1.3.1 jmespath-1.0.1 parsel-1.9.1 protego-0.3.1 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.11.2 service-identity-24.1.0 setuptools-74.1.2 tldextract-5.1.2 tomli-2.0.1 typing-extensions-4.12.2 w3lib-2.2.1\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dffdc7",
   "metadata": {},
   "source": [
    "## Create a Scrapy Project: \n",
    " - Create a new Scrapy project named \"web_scraper\" in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29073772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "After installing a scrapy using above command in my system with the help of command terminal.I went back to my file and\n",
    "created specific file(example quotes) to store the project. \n",
    "And i went back to the command terminal and changed my working directory to the file that i created \n",
    "and run (scrapy startproject web_scraper) to create a Scrapy project.\n",
    "\"\"\"\n",
    "\n",
    "scrapy startproject web_scraper  #create a scrapy project named web_scraper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9387386",
   "metadata": {},
   "source": [
    "## Task 2: Create a Spider to Scrape a Website\n",
    "### Choose a Website: Select a simple, publicly accessible website to scrape.\n",
    "Examples include:\n",
    " - http://quotes.toscrape.com (A website designed for practicing web scraping)\n",
    " - Generate a Spider: Create a spider within your project to scrape the website.\n",
    " - Name the spider based on the website, e.g., quotes_spider for the quotes website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spider inside the web_scraper directory \n",
    "cd web_scraper\n",
    "scrapy genspider quotes quotes.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4e4df",
   "metadata": {},
   "source": [
    "### Extract Data:\n",
    "- Extract the following data from the website:\n",
    "\n",
    "    - Quotes: Extract the text of the quotes.\n",
    "    - Authors: Extract the name of the author for each quote.\n",
    "    - Tags: Extract tags associated with each quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153988ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spider folder quotes_spider.py \n",
    "\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpiderSpider(scrapy.Spider):\n",
    "    name = \"quotes_spider\"\n",
    "    allowed_domains = [\"quotes_spider.py\"]\n",
    "    start_urls = [\"https://quotes.toscrape.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "          for quote in response.css(\"div.quotes\"):\n",
    "                yield{\n",
    "                      \"author\":quote.xpath(\"span/small/text()\").get(),\n",
    "                       \"text\": quote.css(\"span.text::text\").get(),\n",
    "                 }\n",
    "\n",
    "           next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "           if next_gen is not None:\n",
    "                yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935e076",
   "metadata": {},
   "source": [
    "## Task 3: Save the Scraped Data\n",
    "- Save Data to a JSON File: Run the spider and save the scraped data to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60157e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved the Scrapped data using the following code in the spider with the help of notepad inserting following commands\n",
    "scrapy crawl  QuoteSpider -o output.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e3b11",
   "metadata": {},
   "source": [
    " - Save Data to a CSV File: Run the spider again and save the data to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c3acf",
   "metadata": {},
   "source": [
    "### Task 4: Implement Error Handling and Logging\n",
    " - Add Error Handling: Modify your spider to include basic error handling, such as retrying failed requests or skipping certain    elements if they are not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584931bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccef391",
   "metadata": {},
   "source": [
    "- Enable Logging: Configure Scrapy’s logging to monitor your spider’s activity. Write logs to a file for review.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3c57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
